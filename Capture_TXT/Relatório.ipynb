{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/sthima_logo.jfif\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; width:20%;\" />\n",
    "     \n",
    "<img src=\"img/ATF.jfif\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"width:40%;\" />\n",
    "\n",
    "\n",
    "# Score de Prospecção\n",
    "___\n",
    "> ## Resumo\n",
    ">> O trabalho que será apresentado, é referente ao modelo dos dados oriundos de consultas realizadas no SERASA. Tais dados são armazenados no banco de dados da ATF e é composto por dois tipos diferentes, sendo o tipo 1: TXT Limpo e tipo 2: TXT Puro. Sendo assim, os codigos gerados são responsaveis por conseguir imterpretar cada tipo de dado de forma separada, convertendo um arquivo do tipo de texto em diversas tabelas. Além disso, após as tabelas geradas é necessario gerar informação util dessas tabelas, para isso, cada tabela é convertida em diversos outros dados que serão utilizados por um modelo em formato de Árvore. Por fim, o modelo gerado conseguiu uma precisão de cerca de 80%.\n",
    "> ____\n",
    "\n",
    "> ## Os Dados\n",
    ">> A captura dos dados são feitos a partir do banco *serversql.netfactor* da tabela *nfConsultaSerasa*. Após a captura dos dados do tipo 1 e do tipo 2 eles são unificados nos mesmos tipos de tabela sendo elas:\n",
    ">>\n",
    ">> ### REGISTRO DE CONSULTAS\n",
    ">>> DATA, QTD\n",
    ">>\n",
    ">> ### CINCO ULTIMAS CONSULTAS\n",
    ">>> EMPRESA, QTD, DATA\n",
    ">>\n",
    ">> ### HISTORICO DE PAGAMENTOS NO MERCADO\n",
    ">>>  MES/ANO ,  PONTUAL_QTD ,  PONTUAL_% ,  8-15_QTD ,  8-15_% ,  16-30_QTD ,  16-30_% ,  31-60_QTD ,  31-60_% ,  +60_QTD ,  +60_% ,  A_VISTA_QTD ,  A_VISTA_% ,  TOTAL \n",
    ">>\n",
    ">> ### EVOLUCAO DE COMPROMISSOS - VISAO CEDENTE\n",
    ">>> MES/ANO, VENCIDOS, A VENCER, TOTAL\n",
    ">>\n",
    ">> ### PEFIN\n",
    ">>> DATA, MODALIDADE, AVAL, VALOR, CONTRATO, ORIGEM\n",
    ">>\n",
    ">> ### REFIN\n",
    ">>> DATA, MODALIDADE, AVAL, VALOR, CONTRATO, ORIGEM\n",
    ">>\n",
    ">> ### DIVIDA VENCIDA\n",
    ">>> DATA, MODALIDADE, VALOR, TITULO, INST_COBRADORA, LOCAL\n",
    ">>\n",
    ">> ### PROTESTO\n",
    ">>> DATA, VALOR, CARTORIO, CIDADE, UF\n",
    ">>\n",
    ">> ### ACAO JUDICIAL\n",
    ">>> DATA, NATUREZA, AVAL, VALOR, DIST, VARA, CIDADE, UF\n",
    ">>\n",
    ">> ### FALENCIA\n",
    ">>> DATA, TIPO, ORIGEM, CIDADE, UF\n",
    "> ___\n",
    "\n",
    "> ## As *Features*\n",
    ">> Dado as todas as tabelas geradas,cada uma foi observada de forma separada e foi identificado possiveis caracteriscas de cada uma (*Features*), essas características foram isoladas e salvas em um banco a parte, alem disso algumas dessas features são definidas como Tendência e Frequência, sendo a Tendência calculada em cima da media movel e a Frequência calculada em quantidade de dias, sendo a diferença media de dias entre uma linha e outra. Dessa forma, as fetures capturadas são:\n",
    ">>\n",
    ">>\n",
    ">> ### CINCO ULTIMAS CONSULTAS\n",
    ">>> * **1_TOTAL_FACTORINGS** = Total de empresas do tipo \"Factoring\" encontradas;\n",
    ">>> * **1_FREQUENCIA_CONSULTAS** = Frequência entre todas consultas;\n",
    ">>> * **1_FREQUENCIA_CONSULTAS_FACTORING** = Frequência entre todas consultas de empresas do tipo \"Factoring\";\n",
    ">>\n",
    ">> ### REGISTRO DE CONSULTAS\n",
    ">>> * **2_TENDENCIA_CRESCIMENTO** = Se o número de consultas apresenta uma tendencia de crescimento positiva (1 se apresentar, 0 caso contrário);\n",
    ">>> * **2_ACIMA_MEDIA** = Se uma das 3 ultimas linhas possui total de consultas maior do que a média de consultas mais 0.2% da média (1 se apresentar, 0 caso contrário);\n",
    ">>> * **2_TOTAL_CONSULTAS** = A soma do número total de consultas; \n",
    ">>> * **2_TOTAL_CONSULTAS_PONDERADA** = O número de consultas total multiplicado por uma fotor *y* que aumenta de acordo com a atualidade da consulta; \n",
    ">>> * **2_POSSUI_CRESCIMENTO** = Se a quantidade de consultas dos 3 ultimos meses é maior do que a média de consultas mais 0.3% da média (1 se apresentar, 0 caso contrário);\n",
    ">>\n",
    ">> ### REFIN\n",
    ">>> * **3_ULTIMA_MODALIDADE** = Modalidae da dívida mais recente;\n",
    ">>> * **3_MODALIDADE_MAIS_PRESENTE** = Modalidade da dívida mais presente;\n",
    ">>> * **3_FREQUENCIA_DEBITO** = Frequência entre as dividas;\n",
    ">>> * **3_VALOR_DEBITO** = O somatório dos valores dos débitos;\n",
    ">>> * **3_QUANTIDADE_DEBITO** = Total de débitos presentes. \n",
    ">>\n",
    ">> ### PEFIN\n",
    ">>> * **4_ULTIMA_MODALIDADE** = *Explicado anteriormente;* \n",
    ">>> * **4_MODALIDADE_MAIS_PRESENTE** = *Explicado anteriormente;* \n",
    ">>> * **4_FREQUENCIA_DEBITO** = *Explicado anteriormente;* \n",
    ">>> * **4_VALOR_DEBITO** = *Explicado anteriormente;* \n",
    ">>> * **4_QUANTIDADE_DEBITO** = *Explicado anteriormente;* \n",
    ">>> * **4_TOTAL_FACTORINGS_DEBITO** = Total de débitos referentes a empresas do tipo \"Factoring\";\n",
    ">>\n",
    ">> ### DIVIDA VENCIDA\n",
    ">>> * **5_ULTIMA_MODALIDADE** = *Explicado anteriormente;*\n",
    ">>> * **5_MODALIDADE_MAIS_PRESENTE** = *Explicado anteriormente;*\n",
    ">>> * **5_FREQUENCIA_DEBITO** = *Explicado anteriormente;*\n",
    ">>> * **5_VALOR_DEBITO** = *Explicado anteriormente;*\n",
    ">>> * **5_QUANTIDADE_DEBITO** = *Explicado anteriormente;*\n",
    ">>\n",
    ">> ### HISTORICO DE PAGAMENTOS NO MERCADO\n",
    ">>> * **6_TOTAL_PAGAMENTOS** = Total de pagamentos realizados;\n",
    ">>> * **6_PRESENCA_PAGAMENTOS** = Somatorio da quantidade de pagamentos realizados sendo 8-15 com um peso de 0.3, 16-30 com peso de 0.5, 31-60 com peso de 0.7 e +60 com peso de 0.9\n",
    ">>>\n",
    ">>> Cada uma das carácteristicas abaixo se repete para todos os periodos de tempo da tabela (8-15, 16-30, 31-60 e +60):\n",
    ">>> * **6_PAGAMENTO_PERCENT_*x*** = Percentual da tabela composto por dividas pagas no periodo de tempo *x*;\n",
    ">>> * **6_PAGAMENTO_VALOR_*x*** = Valor total de pagamentos no periodo de tempo *x*;\n",
    ">>> * **6_PAGAMENTO_TEND_CRES_*x*** = Se os pagamentos no periodo de tempo *x* apresentam uma tendencia de crescimento;\n",
    ">>\n",
    ">> ### EVOLUCAO DE COMPROMISSOS - VISAO CEDENTE\n",
    ">>> * **7_TOTAL_COMMITMENTS** = Total de compromissos;\n",
    ">>> * **7_POSSUI_CRESCIMENTO** = O Somatório da quantidade de VENCIDOS e A VENCER, sendo as três primeiras linhas com um peso de 0.8 e o restante de 0.2;\n",
    ">>\n",
    ">>> Cada uma das carácteristicas abaixo se repete para cada uma das categoriás de compromissos, sendo \"VENCIDOS\" e \"A VENCER\":\n",
    ">>> * **7_TEND_CRESCIMENTO_*categoria*** =  Se o valor apresenta uma tendencia de crescimento positiva (1 se apresentar, 0 caso contrário);;\n",
    ">>> * **7_VALOR_TOTAL_*categoria*** = O somatorio total dos valores de determinada categoria;\n",
    ">>\n",
    ">> ### FALENCIA\n",
    ">>> * **8_TOTAL_FALENCIA_REQ** = Total de falencias do tipo REQUERIDA\n",
    ">>> * **8_TOTAL_FALENCIA__CONC** = Total de falencias do tipo CONCEDIDA\n",
    ">>\n",
    ">> ### ACAO JUDICIAL\n",
    ">>> * **9_NATUREZA_MAIS_PRESENTE** = A natureza da ação judicial mais presente;\n",
    ">>> * **9_TOTAL_ACAO_JUDICIAL** = O total de ações judiciais;\n",
    ">>> * **9_VALOR_TOTAL** = A soma dos valores totais;\n",
    ">>> * **9_FREQUENCIA_ACAO_JUDICIAL** = A frequência de ocorrencia de ação judicial\n",
    ">>\n",
    ">> ### PROTESTO\n",
    ">>> * **10_TOTAL_PROTESTOS** = O numero total de protestos;\n",
    ">>> * **10_STD_VALOR** = O desvio padrão dos valores dos protestos;\n",
    ">>> * **10_MEDIA_VALOR** = A media dos valores dos protestos;\n",
    ">>> * **10_FREQUENCIA_PROTESTO** = Frequência entre todos os protestos; \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "## Metodologia \n",
    "\n",
    "No fim do processo de tratamento dos dados foram geradas 60 novas features para a base. Após essa etapa é necessario identificar as melhores features para serem utilizadas pelo modelo de Aprendizado de máquina, para isso após realizar a tentativa de seleção de features pelo calculo do Peso de Evidencia nao foram encontrados bons resultados. Sendo assim, foi utilizado um metodo de diminuisão de dimensionalidade chamado PCA. \n",
    "\n",
    "### *Principal Component Analyze* (PCA)\n",
    "O PCA é um metódo baseado operações vetoriais que busca identificar e simplificar determinado conjunto de dados sem perder informação. É um metodo poderoso que pode reduzir um conjunto de dados de 60 colunas para quantas colunas forem necessárias mantendo o padrão dos dados. Entretanto é necessário encontrar o numéro certo de colunas que possa representar todas as features creadas. Para isso é utilizado o metodo de *elbow*, ou metodo do cutuvelo, que é definido por observar no gráfico de *elbow* e identificar o momento onde se para de ter um grande ganho em diminuir a proporção da variancia entre as variaveis. Ou seja, a partir daquele ponto as variaveis novas já teriam sido \"explicadas\" por outras ja criadas. \n",
    "\n",
    "<img src=\"img/elbow.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"width:40%;\" />\n",
    "     \n",
    "No nosso gráfico de elbow gerado é possivel identificar que o melhor número de colunas que possa manter toda a explicação dos nossos dados é de **5 colunas**. Além disso, vale a pena salientar que para o metodo de PCA ser utilizado é necessário normalizar os dados, sendo assim, todas as colunas numéricas foram normalizadas entre 0 e 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Aprendizado\n",
    "Depois de todo a etapa de tratamento e escolha das features, foram utilizados 804 dados para essa etapa de treinamento do modelo, sendo 201 para teste. O modelo utilizado foi o de árvore de decisão, tendo em vista que modelos em árvore tem demonstrado um comportamento melhor com relação a presença de *outliers*, tendo em vista que a base disponibilizada poderia conter falsos positvos. Entretanto outros modelos foram testados, como regressão logística e outros modelos de *boost*. Por fim, para encontrar os melhores parámetros para o modelo foi utilizado a meta-heuristica *Simulated Annealing* ou Temperatura Simulada, que nada mais é do que um método de otimização de soluções em um grande espaço de busca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo apresentado nessa etapa do projeto demonstou uma precisão bem alta de 80%, em deterimento do *recall* e da *accuracy*, ou seja, os resultados que ele afirma ser possitivo possuem uma alta confiabilidade. Podendo ser observado abaixo a matriz de confussão: \n",
    "\n",
    "<img src=\"img/Matriz de Confusão.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"width:50%;\" />\n",
    "     \n",
    "Outro ponto a ser observado deve ser também a curva de aprendizado do modelo, nela pode ser observado a variação da precisão com tamanho da base encontrado:\n",
    "\n",
    "<img src=\"img/Curva de Aprendizado.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"width:50%;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nela podemos observar o quanto um conjunto pequeno de dados pode ser distuante um do outro, pois a precisão do treino é muito distinta da precisão do conjunto de validação. Por outro lado, podemos obervar tambem que com o aumento do conjunto de treino, a precisão começa a aumentar, demonstrando que ainda a espaço para melhora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "276.027px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
